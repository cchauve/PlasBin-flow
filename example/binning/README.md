# Preprocessing and processing samples for PlasBin-flow

The file `input.csv` contains an example (composed of two
assemblies of a *Pseudomonas aeruginosa* sample, one Unicycler
assembly and one SKESA assembly) of running PlasBin-flow
preprocessing and binning commands.

## Input data

For each of the samples file, the initial data contains
- an assembly GFA file (`<sample>.gfa.gz`),
- a FASTA file obtained from the GFA file (`<sample>.fasta`) using the command
  `python example_utils.py create_FASTA data/<sample>.gfa.gz data/<sample>.fasta`.
  

Then, for each sample, plasmid scores were obtained with three
different plasmid contigs classifiers:
<a href="https://github.com/Shamir-Lab/PlasClass">PlasClass</a>,
<a href="https://github.com/cchauve/plASgraph2/">plASgraph2</a>,
<a href="https://github.com/aldertzomer/RFPlasmid">RFPlasmid</a>
(using its *Pseudomonas* model).  For RFPlasmid, as it does not work
with very short contigs, the FASTA file was filtered to eliminate all
contigs shorter than 100bp using the command `python example_utils.py
filter_FASTA data/<sample>.fasta data/<sample>.RFPlasmid.fasta 100`.

The contigs classification files generated by these tools are
respectively in the files `data/<sample>.<tool>.pred.csv` with `tool`
in `[PlasClass, plASgraph, RFPlasmid]`. They were reformated into the
plasmid score format expected by PlasBin-flow by the command
`
python example_utils.py reformat_<tool> <sample>.<tool>.pred.csv <sample>.<tool>.pls_score.csv
`.

The file `input.csv` contains the path to access the GFA file for each
sample as well as the plasmid scores from the three classification
tools used.

## Preprocessing

Then the two samples were preprocessed to compute the gene density
(using the default plasmid genes database) and the GC content
probabilities using the default GC content intervals.

```
python ../../code/plasbin_utils.py preprocessing \
       --input_file    input.csv \
       --out_dir       preprocessing_output \
       --tmp_dir       preprocessing_tmp \
       --log_file      preprocessing.log \
       --out_file      preprocessing_output.csv \
       --db_file       ../../database/genes.fasta \
       --pid_threshold 0.9 \
       --cov_threshold 0.7
```

The optional parameters `pid_threshold 0.9` is non-default (default
value = 0.95), as is `--cov_threshold 0.7` (default value = 0.8.

The file `preprocessing_output.csv` will contain fields for the
contigs plasmid score based on gene density and GC content
probabilities files for both samples.

The temporary directory `preprocessing_tmp` is deleted but if the
optional parameter `--keep_tmp_dir` is also used.

The main results will be the files
- `preprocessing_output/<sample>.gc.tsv`: GC content probabilities file;
- `preprocessing_output/<sample>.gd.tsv`: plasmid scores (gene density) file.

Additionally, for each sample, the following files will be created:
- `<sample>.genes_mappings.tsv`: mapping of reference plasmid
  genes to sample contigs;
- `<sample>.gd.tsv`: gene density file (used as contigs plasmid score
  file).

The file `preprocessing.log` contains a detailed log.

After the preprocessing is completed, the samples can be processed to
compute plasmid bins, using the default GC intervals. As there are 4
different ways to define the plasmid score (from the gene density and
the three classification tools), we generate 4 different plasmid bins
file per sample. Moreover, to account for the unreliability of plasmid
scores for short contigs, we assign the default plasmid score (0.5) to
all contigs of length shorter than 100bp (parameter `-min_ctg_len
100`), a time limit for Gurobi of 10 minutes (parameter
`-gurobi_time_limit 600`) and an upper bound of at most 20 iterations
of Gurobi to exclude circular components (parameter `rmiter_max 20`);
this parameters choice is intended to illustrate their use,
although we recommend to use the default values unless Gurobi
takes a long time to complete, which indicates likely a sample with a
very tangled assembly graph.

The bash script below generates the plasmid bins predictions.
```
#!/bin/bash

find_pos () {
    HEADER=`head -1 $1 | sed 's/,/ /g'`
    COL_NAME=$2
    PREFIX=${HEADER%%$COL_NAME*}
    COL_POS=`echo ${PREFIX} | awk '{print NF+1}'`
    return ${COL_POS}
}

INPUT=preprocessing_output.csv
OUT_DIR=binning_output
SCORE_SOURCES="pls_score PlasClass RFPlasmid plASgraph"

for TASK_ID in {1..2};
do
    SAMPLE_LINE=`expr ${TASK_ID} + 1`

    for SCORE_SOURCE in ${SCORE_SOURCES};
    do
	find_pos ${INPUT} sample
	SAMPLE=$(sed -n "${SAMPLE_LINE}p" ${INPUT} | cut -f$? -d',')
	find_pos ${INPUT} assembler
	ASSEMBLER=$(sed -n "${SAMPLE_LINE}p" ${INPUT} | cut -f$? -d ',')
	find_pos ${INPUT} gfa
	GFA=$(sed -n "${SAMPLE_LINE}p" ${INPUT} | cut -f$? -d ',')
	find_pos ${INPUT} gc_probabilities
	GC=$(sed -n "${SAMPLE_LINE}p" ${INPUT} | cut -f$? -d ',')
	find_pos ${INPUT} ${SCORE_SOURCE}
	GD=$(sed -n "${SAMPLE_LINE}p" ${INPUT} | cut -f$? -d ',')
	
	python ../../code/plasbin_flow.py \
	       -ag ${GFA} \
	       -gc ${GC} \
	       -score ${GD} \
	       -out_dir ${OUT_DIR} \
	       -out_file ${SAMPLE}.${SCORE_SOURCE}.pred.txt \
	       -log_file binning_${SAMPLE}.${SCORE_SOURCE}.log \
	       -assembler ${ASSEMBLER} \
	       -min_ctg_len 100 \
	       -gurobi_time_limit 600 \
	       -rmiter_max 20
    done

    # Renaming the plasmid bins file and log file obtained with gene density for plasmid score
    mv ${SAMPLE}.pls_score.pred.txt ${SAMPLE}.gene_density.pred.txt
    mv ${SAMPLE}.pls_score.log ${SAMPLE}.gene_density.log
done
```

This script uses a BASH loop and process both samples iteratively, but
samples could be processed in parallel on a HPC system, using for
example the array feature of the SLURM scheduler.

The output of the binning command for each sample are the files
`binning_output/<sample>.<tool>.pred.txt`
where `tool` is in `[gene_density,PlasClass,plASgraph,RFPlasmid]`.
